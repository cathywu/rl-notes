{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reward Shaping Demo\n",
    "The goal of this notebook is to apply concepts of reward shaping to more efficiently solve Markov Decision Processes (MDPs). We will see that:\n",
    "\n",
    "1. A well-designed potential function **accelerates** solving the MDP, leading to the agent reaching its goal faster.\n",
    "2. A poorly-designed potential function **slows** solving the MDP, leading to the agent reaching its goal slower.\n",
    "3. Reward shaping that is not a potential function may result in **not solving** the MDP, leading to convergence to the wrong value."
   ]
  },
  {
   "attachments": {
    "basic-gridworld.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjYAAAGtCAYAAAAF/z4oAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAYvElEQVR4nO3dfXDcd33g8c/uypIs68HCsRPLT9g4tknihDhpYnNNaC6BAD3uuEJugJbOcIUczLTkjimdoYX+Uej15to5joGbzlG45hJaoASaFAotNO0EwtlJsBPyHCcYP8iy4ydZsizrabX3R2ITxU+SLXm1H71eMxlPfvv77n52suN957e/3V+hUqlUAgAggWK1BwAAmCzCBgBIQ9gAAGkIGwAgDWEDAKQhbACANIQNAJCGsAEA0qgbz06jo6PR1dUVLS0tUSgUpnomAIATKpVKHDlyJDo6OqJYPPMxmXGFTVdXVyxZsmRShgMAOBe7du2KxYsXn3GfcYVNS0tLRERs37492tvbz38y0imXy/Hcc8/F6tWro1QqVXscphmvD87mI/d8JH7nst+Jzz/9+RgeHa72OEwzw8eG496P3nuiR85kXGFz/OOn1tbWaG1tPb/pSKlcLkdzc3O0trZ64+IkXh+cTX1TfTQ3N0d9U33EaLWnYboaz+kwTh4GANIQNgBAGsIGAEhD2AAAaQgbACANYQMApCFsAIA0hA0AkIawAQDSEDYAQBrCBgBIQ9gAAGkIGwAgDWEDAKQhbACANIQNAJCGsAEA0hA2AEAawgYASEPYAABpCBsAIA1hAwCkIWwAgDSEDQCQhrABANIQNgBAGsIGAEhD2AAAaQgbACANYQMApCFsAIA0hA0AkIawAQDSEDYAQBrCBgBIQ9gAAGkIGwAgDWEDAKQhbACANIQNAJCGsAEA0hA2AEAawgYASEPYAABpCBsAIA1hAwCkIWwAgDSEDQCQhrABANIQNgBAGsIGAEhD2AAAaQgbACANYQMApCFsAIA0hA0AkIawAQDSqKv2AACQ2ejIaGz9p61xeMfhOLT9UPTu7o3R8mhc98HrYuVNK8/pPvdv3R9P3vtkHHzhYJSHytFySUuseNOKWHXrqigWT33MYveW3fHMd5+J7u3dURmtRNvitrj0lktjxY0rzufpTTvCBgCm0MjgSGy5e0tERDS2NUbj3MboP9h/zvfX+ZPO+NHnfhSlWaVYun5pNDQ3xO4tu2PLV7bE/q3744Y7bjhpzXPffy42/9/N0dDcEK/9V6+NYl0xdj28Kzb9701xeNfhWPfr6855nulG2ADAOdj2wLbY9MVNcfMf3BwXX3bxafcrNZTiVz7+K9G+rD1mt8+Ox7/5eDz5rSfP6TGH+4fjoS8/FIViIW7+5M0xb8W8iIi48t1Xxv3/9f7Y9fCu2L5xe7x2w2tPrOnb3xeP/vWjUd9cH7d+5tZont8cERFrf21t/MOn/iGe/e6zseS6JTH/0vnnNNN04xwbAJhCpbpSdLyhI2a3zz7v+9r58M4Y7B2MZRuWnYiaiIhSfSmuvO3KiIh44Z9eGLNm2wPbYnR4NFa9edWJqImIqJ9TH5f/28tfWnP/2DW1TNgAQI148ekXIyJi4ZULT7ptwZoFUWooxf7n90d5uPyLNU+9vOaqk9d0XNUxZp8MhA0A1IjePb0REdG6sPWk24qlYjTPb45KuRJ9+/pOXnPJyWtmt8+Ouoa66D/UHyODI1M09YUlbACgRgz3D0dExKzZs055+/HtQ/1DJ69pOs2al7cf36/WOXkYAM7ivjvui6MHjp7ytvv/+P6Tti2/YXls+PCGqR6LUxA2AHAWq9+6esxRkIiIwzsOR+fmzlh+w/KYM3/OmNval7VPyRwnjq4cO/XRlePb65vqx6wZPDIYw/3D0dDScPKasxzRqTXCBgDOYs3b1py0bdsD26Jzc2esuHHFGb/uPZlaF7bGoW2HondPb7xm+WvG3DZaHo2+/X1RKBWieUHzmDX7j+yP3r29Mb9l7Fe6j3Ufi5HBkWh6TVPUNeRIAufYAECNOB5Qex7fc9Jt+57dF+XBcsy/dH6UZpV+sebyl9f89OQ1XT/tGrNPBsIGAKaZof6h6OnqiWPdx8ZsX3rd0mhoaYgdG3fEwW0HT2wvD5Xj8W88HhERK28Ze5mGFTeuiOKsYmz9wdbo2/+Lb0sNHR2Kp/7uqZfW3Hxul3aYjnIcdwKAaeypv3sqerte+tp1947uiHjpo6z9z+2PiIj5q+ePuW5U5yOdsemLm046CXlW06y47oPXxYOfezDu/8z9sWzDsqifUx+7t+yO3j29seS6JbFs/bIxj928oDmufu/VsfmuzfGPn/zHWLp+6YlLKvQf6o81b1+T5leHI4QNAEy5PY/viX3P7Buz7cDzB+LA8wdO/Pt4L4i55Nolccsnb4kn73sydj68M0aHR6P54uZY9xvrYtWtq6JQKJy0ZvWtq2PO/Dnx7N8/Gz9/8OdRqVSibVFbXHnblS6CCQBErHjTiljxpvFFwS2fvGVS73v+6vlx0+/dNKH7XLxucSxet3hCa2qRc2wAgDSEDQCQhrABANIQNgBAGsIGAEhD2AAAaQgbACANYQMApCFsAIA0hA0AkIawAQDSEDYAQBrCBgBIQ9gAAGkIGwAgDWEDAKQhbACANIQNAJCGsAEA0hA2AEAawgYASEPYAABpCBsAIA1hAwCkIWwAgDSEDQCQhrABANIQNgBAGsIGAEhD2AAAaQgbACANYQMApCFsAIA0hA0AkIawAQDSEDYAQBrCBgBIQ9gAAGkIGwAgDWEDAKQhbACANIQNAJCGsAEA0hA2AEAawgYASEPYAABpCBsAIA1hAwCkIWwAgDSEDQCQhrABANIQNgBAGsIGAEhD2AAAaQgbACANYQMApCFsAIA0hA0AkIawAQDSEDYAQBp1E9m5XC5HuVyeqlmoYcdfF14fnIrXB2dTX6wf8yeMMYHDMIVKpVI52069vb3R1tYWGzdujObm5vMZDQBgQvr6+mLDhg3R09MTra2tZ9x3QkdsVq5cGe3t7ec1HDndeeedcf3118dDDz3k/8o5SalUiuuvvz5WrVoVpVKp2uMwDf3kLW+JOZ/7XBy9446IgYFqj8M00z8yMu59JxQ2pVLJX0qc0is/ahA2nI6/Qzit4zEzMCBsONkE3lecPAwApCFsAIA0hA0AkIawAQDSEDYAQBrCBgBIQ9gAAGkIGwAgDWEDAKQhbACANIQNAJCGsAEA0hA2AEAawgYASEPYAABpCBsAIA1hAwCkIWwAgDSEDQCQhrABANIQNgBAGsIGAEhD2AAAaQgbACANYQMApCFsAIA0hA0AkIawAQDSEDYAQBrCBgBIQ9gAAGkIGwAgDWEDAKQhbACANIQNAJCGsAEA0hA2AEAawgYASEPYAABpCBsAIA1hAwCkIWwAgDSEDQCQhrABANIQNgBAGsIGAEhD2AAAaQgbACANYQMApCFsAIA0hA0AkIawAQDSEDYAQBrCBgBIQ9gAAGkIGwAgDWEDAKRRV+0BACCbp44ejQcOH46tx47F1v7+ODQyEvNnzYpvr117Tvc3MDoad+/dGz/o7o69Q0Mxp1SKdc3N8cGFC2P57NmnXNMzMhL/Z8+eeKCnJw4OD0dbXV2sb22N2xcujAX19efz9KY1YQMAk+z7hw7F1/fvj7pCIZY3NsahkZFzvq+h0dH46PPPx+NHj8brm5riPyxYEPuGhuL+7u74cW9vfOHSS+OKOXPGrOkZGYkPPfdc7BwcjGtbWuLN7e2xY2AgvnPwYPy4pye+tHp1LGpoON+nOS0JGwCYZL86b168fd68WNHYGLOKxVi/Zcs539dX9+2Lx48ejX89d258ZvnyKBYKERFxS3t7/N62bfHHO3bEX73+9Se2R0T8eVdX7BwcjPcuWBB3LF58YvvX9+2Lz3Z2xp/u2hX/c+XKc3+C05hzbABgkq1qaorVTU0xq3h+b7OVSiX+9sCBiIj47UWLxsTLjXPnxhuam+PnAwPxaF/fie395XJ87+DBmF0sxgcXLhxzf7fNnx+X1NfHpt7e2D04eF6zTVfCBgCmqc7Bwdg7NBRLGxqi4xQfHW1obY2IiJ8cOXJi25NHj8ZgpRJXzpkTc0qlMfsXC4VY//Kaza9Yk4mwAYBpaufLR1WWnOZ8mOPbdw4MnLymsfHMaxyxAQAupL5yOSIiml915OW440dkju93rmsycfIwAEzQX3R1nbTtV+fNO+XHRVxYwgYAJujLe/eetG1dS8ukh03zWY6uHD3F0ZlzWZOJsAGACdq0bt0FeZylL4fSrtOcD3N8+9JXnE9zYs0rzrs55ZqkR5ecYwMA09Tihoa4pL4+dg4ORtcp4mZjb29ERFzb0nJi2xVz5kRDoRCPHz164ujMcaOVSjz08pprXrEmE2EDANNA5+BgbB8YiJFK5cS2QqEQ//6iiyIi4gu7d8foK2774eHD8VhfXyxvbIyrm5tPbG8qleJt8+bFsdHR+NKePWMe4xv798eeoaFY39rql4cBgPHZPjAQd73qPJwj5XL80fbtJ/79o4sXx9y6X7wN//bzz8feoaH41uWXjzlX570LFsSDPT3xz4cPx28991xc29ISL758SYXGYjH+YNmyMT/cFxHxkY6O2HLkSHx13754/tixuKypKbYPDMQPe3qiva4ufnfJkql54tOAsAGASXZweDi+e+jQmG0Do6Njtn1w4cIxYXM69cVifP7SS+OuvXvj+93d8bV9+2JOqRRvmjs3PnSai2C21dXFX6xeHV/esyd+2NMTj/X1RVupFP9m3jwXwQQAJuaalpYJn2B87xVXnPa2xmIxbu/oiNs7OsZ9f211dfGxJUviY4mPzpyKc2wAgDSEDQCQhrABANIQNgBAGsIGAEhD2AAAaQgbACANYQMApCFsAIA0hA0AkIawAQDSEDYAQBrCBgBIQ9gAAGkIGwAgDWEDAKQhbACANIQNAJCGsAEA0hA2AEAawgYASEPYAABpCBsAIA1hAwCkIWwAgDSEDQCQhrABANIQNgBAGsIGAEhD2AAAaQgbACANYQMApCFsAIA0hA0AkIawAQDSEDYAQBrCBgBIQ9gAAGkIGwAgDWEDAKQhbACANIQNAJCGsAEA0hA2AEAawgYASEPYAABpCBsAIA1hAwCkIWwAgDSEDQCQhrABANIQNgBAGsIGAEhD2AAAaQgbACANYQMApCFsAIA0hA0AkIawAQDSEDYAQBrCBgBIo24iO5fL5SiXy1M1CzWsVCqN+RNe6fjrwt8fnFZj49g/4ZVGRsa9a6FSqVTOtlNvb2+0tbXFxo0bo7m5+bxmAwCYiL6+vtiwYUP09PREa2vrGfed0BGblStXRnt7+3kNR07lcjm2bt0aq1atctSGk3h9cDZeI5xJb2/vuPedUNiUSiUvOM7Ia4Qz8frgbLxGOJWJvCacPAwApCFsAIA0hA0AkIawAQDSEDYAQBrCBgBIQ9gAAGkIGwAgDWEDAKQhbACANIQNAJCGsAEA0hA2AEAawgYASEPYAABpCBsAIA1hAwCkIWwAgDSEDQCQhrABANIQNgBAGsIGAEhD2AAAaQgbACANYQMApCFsAIA0hA0AkIawAQDSEDYAQBrCBgBIQ9gAAGkIGwAgDWEDAKRRV+0BasXh/qHYfrA/BobLMTBcjtFKJepLpWicVYy5TbNi+UXNUSoWqj0mAMxowuYUDvcPxRO7e176p/OlPzu7j51xzexZpbisozXWLmqLKxa1xdpFbbFygdgBgAtJ2LysPFqJ+595Me7etCMefOFAVCoTW39suBybd3TH5h3dJ7Zd1NwQ7/mlJfG+65dGx9zZkzwxAPBqMz5sDvQNxtce3hlffXhX7D585qMy53LfX/iXF+LPH/hZ3LxmQbx/w7L45ZUXRaHgKA4ATIUZGzY9x4bjv33vmfjm5t0xVB6d0scqj1bi+0+/GN9/+sVYcdGc+NQ7LoubVi+Y0scEgJloRn4r6p+ffTFu/ewP46sP75ryqHm1bQeOxgf+8pH43W/8NHqODV/QxwaA7GbUEZueY8PxR99+Or65pbPao8Q9mzvjwecPxJ/82tq4aY2jNwAwGWbMEZsfPb8/3vLZB6ZF1By3t3cgPnDnS0dvjg2Vqz0OANS8GRE29z66Oz7wl4/Ei72D1R7llO7Z3Bnv//JDPpoCgPOUPmy+smlH/Je/eSxGRif4/e0L7Cc7uuM9X9wUB/qmZ3wBQC1IHTZ/88iu+NR9T074N2mq5Zk9vfGbX37YkRsAOEdpw+Z7T+yJT/ztEzUTNcc9vac3/uOdjzjnBgDOQcqweWFfX/znrz8W5Wn+8dPpbN7RHZ+678lqjwEANSdd2JRHK/Hxe34agyMX9vdpJts9mzvjX57dV+0xAKCmpAubL/1oWzy683C1x5gUn/jWE9E74HwbABivVGHzwr6++B8/2FrtMSbN3t6B+PS3n672GABQM9KETZaPoF7tGz6SAoBxSxM233m8K81HUK/26e88HZVa+3oXAFRBmrC5a+OOao8wZbYdOBo/fuFgtccAgGkvRdg81dUTm3d0V3uMKXXXxu3VHgEApr0UYfOVTXmP1hx3/7P7Yk/PsWqPAQDTWs2HTe/AcNz3WFe1x5hy5dFK/PVDO6s9BgBMazUfNvc+ujv6Z8jlB772yK6a/TVlALgQaj5sNv5s5pxUu//IYPxsf1+1xwCAaavmw+aJ3T3VHuGCeqJzZj1fAJiImg6bw/1D0dk9s06onWkhBwATUdNhMxPf5J+cgc8ZAMZL2NSYp/f0xqgTiAHglGo6bJ7q6q32CBdc/1A5th04Wu0xAGBaqumw6ekfrvYIVdFzbKjaIwDAtFTTYTMwPDN+v+bVBoZzXcEcACZLTYfNUHlmvsEPjszMoAOAs6npsCkWCtUeoSpKxZr+zwYAU6am3yEbZ9X0+OessW5mPm8AOJuafodsnFWq9ghVMVOfNwCcTU2HzWvnzan2CFWxbF5TtUcAgGmppsPmikVt1R7hglvymtkxt6m+2mMAwLRU02GzdgaGzUx8zgAwXjUdNisXNMfsGXa+ydpFc6s9AgBMWzUdNqViIS7raK32GBeUIzYAcHo1HTYRM+uNvlCIuGLRzAo5AJiImg+bd1zVUe0RLpgbLp3vxGEAOIOaD5trlrXH5TPk46j3r19W7REAYFqr+bCJiPiNGfCGv2ju7Lh5zYJqjwEA01qKsHnnGxZFa2NdtceYUu+7fmkUizPz2lgAMF4pwmZ2fSnedc3iao8xZepLxXjPLy2p9hgAMO2lCJuIiA+8cXk0JL045LuuWRzzmhuqPQYATHtpSmDpvKb42JtXVXuMSXdJa2N84u1rqj0GANSENGETEfGhG1bE1UvnVnuMSfUn71obrY2zqj0GANSEVGFTLBbiT999VZqPpN59zeK4abVvQgHAeOUogFdYuaA5xUdSl7Q2xh++47JqjwEANSVd2ES89JHUG183r9pjnLNZpUL82W1X+QgKACYoZdgUi4X44m9eG1ctrr3rSBULEX9221Xxy5deVO1RAKDmpAybiIjmhrq48wPXxeqLW6o9yrgVChGffucV8e/esKjaowBATUobNhER7XPq42u3r6+JIzelYiH++7uujF+/Pv/lIQBgqqQOm4iX4uavPrR+Wp9z01BXjP/1vqvjtmv9ujAAnI/0YRPx0sdSd//W9fGJt62Zdl8FX7d0bnz3jhvirVcsrPYoAFDzpte7/BQqFQvxn970uvj7j94wLX7Er6GuGL//9jVxz4ffGK+b31ztcQAghRkTNsetXNAc3/zwG+P33169ozfHj9LcfuPrXLEbACZRXbUHqIZisRC33/i6eOvlC+PO/7c97tm8K3oHRqb8ca9d1h7v37As3nFlh6ABgCkwI8PmuKXzmuIP33FZfPzW1XHfY7vj7k074qmu3kl9jKb6Urzz6kXx/vXL4vULWyf1vgGAsWZ02Bw3u74U77luabznuqWxZWd3fPunXfFEZ088vac3+ofKE76/Ja+ZHWsXtcWGFfPinVcviha/IAwAF4SweZV1S9tj3dL2iIgYHa3EC/v74onOnnhid0/sOHg0BoZHY2CkHOXRSjTUFaNxVinaZs+KyzvaYu2itrhiUWvMbaqv8rMAgJlJ2JxBsViIVRe3xKqLW+Jd1yyu9jgAwFnMuG9FAQB5CRsAIA1hAwCkIWwAgDSEDQCQhrABANIQNgBAGsIGAEhD2AAAaQgbACANYQMApCFsAIA0hA0AkIawAQDSEDYAQBrCBgBIQ9gAAGkIGwAgDWEDAKQhbACANIQNAJCGsAEA0hA2AEAawgYASEPYAABpCBsAIA1hAwCkIWwAgDSEDQCQhrABANIQNgBAGsIGAEhD2AAAaQgbACANYQMApFE3np0qlUpERBw+fHgqZ6GGlcvl6Ovri+7u7iiVStUeh2nG64Oz8RrhTHp7eyPiFz1yJuMKmyNHjkRExIoVK85jLACAc3fkyJFoa2s74z6FyjjyZ3R0NLq6uqKlpSUKhcKkDQgAcDaVSiWOHDkSHR0dUSye+SyacYUNAEAtcPIwAJCGsAEA0hA2AEAawgYASEPYAABpCBsAIA1hAwCk8f8BSsqp0EXBzuQAAAAASUVORK5CYII="
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid world, revisited\n",
    "We will revisit the grid world, but now with more states, to make the problem harder. A few items to note:\n",
    "- An episode terminates when the agent reaches a goal state, either good (+1) or bad (-1).\n",
    "- By default, Q-learning will run for 100 episodes. How much can the agent learn in 100 episodes?\n",
    "- We should expect a random walk from the lower left corner to the top right corner to take a while, especially for large grids.\n",
    "- In comparison, with sufficient \"fake rewards\" that nudge the agent in the right direction, we should expect an agent with a shaped reward to quickly find a short path to the good goal state - regardless of how big the grid is.\n",
    "![basic-gridworld.png](attachment:basic-gridworld.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A good potential function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement potential-based reward shaping, we need to first implement a potential function. We implement potential functions as subclasses of ``PotentialFunction``. For the GridWorld example, the potential function is 1 minus the normalised distance from the goal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(1392020)\n",
    "\n",
    "import os\n",
    "import sys\n",
    "directory_path = os.path.abspath(os.path.join('..'))\n",
    "if directory_path not in sys.path:\n",
    "    sys.path.append(directory_path)\n",
    "\n",
    "from rl_notes.gridworld import GridWorld\n",
    "from rl_notes.potential_function import PotentialFunction\n",
    "\n",
    "\n",
    "class GridWorldPotentialFunction(PotentialFunction):\n",
    "    def __init__(self, mdp):\n",
    "        self.mdp = mdp\n",
    "\n",
    "    def get_potential(self, state):\n",
    "        if state != GridWorld.TERMINAL:\n",
    "            goal = (self.mdp.width, self.mdp.height)\n",
    "            x = 0\n",
    "            y = 1\n",
    "            return 0.1 * (\n",
    "                1 - ((goal[x] - state[x] + goal[y] - state[y]) / (goal[x] + goal[y]))\n",
    "            )\n",
    "        else:\n",
    "            return 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reward shaping for Q-learning is then a simple extension of the ``QLearning`` class, overriding the ``update`` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_free_reinforcement_learner import ModelFreeReinforcementLearner\n",
    "from qlearning import QLearning\n",
    "\n",
    "\n",
    "class RewardShapedQLearning(QLearning):\n",
    "    def __init__(self, mdp, bandit, potential, qfunction, alpha=0.1):\n",
    "        super().__init__(mdp, bandit, qfunction=qfunction, alpha=alpha)\n",
    "        self.potential = potential\n",
    "\n",
    "    def get_delta(self, reward, q_value, state, next_state, next_action):\n",
    "        next_state_value = self.state_value(next_state, next_action)\n",
    "        state_potential = self.potential.get_potential(state)\n",
    "        next_state_potential = self.potential.get_potential(next_state)\n",
    "        potential = self.mdp.discount_factor * next_state_potential - state_potential\n",
    "        delta = reward + potential + self.mdp.discount_factor * next_state_value - q_value\n",
    "        return delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can run this on a GridWorld example with more states:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'QTable' object has no attribute 'extract_policy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m RewardShapedQLearning(mdp, EpsilonGreedy(), potential, qfunction)\u001b[38;5;241m.\u001b[39mexecute()\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# RewardShapedQLearning(mdp, EpsilonGreedy(), potential, qfunction).execute(visualize=True)\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m policy \u001b[38;5;241m=\u001b[39m qfunction\u001b[38;5;241m.\u001b[39mextract_policy(mdp)\n\u001b[1;32m     14\u001b[0m mdp\u001b[38;5;241m.\u001b[39mvisualise_q_function(qfunction)\n\u001b[1;32m     15\u001b[0m mdp\u001b[38;5;241m.\u001b[39mvisualise_policy(policy)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'QTable' object has no attribute 'extract_policy'"
     ]
    }
   ],
   "source": [
    "from qtable import QTable\n",
    "from qlearning import QLearning\n",
    "from reward_shaped_qlearning import RewardShapedQLearning\n",
    "from gridworld_potential_function import GridWorldPotentialFunction\n",
    "from multi_armed_bandit.epsilon_greedy import EpsilonGreedy\n",
    "\n",
    "\n",
    "mdp = GridWorld(width = 10, height = 7, goals = [((9,6), 1), ((8,6), -1)])\n",
    "qfunction = QTable()\n",
    "potential = GridWorldPotentialFunction(mdp)\n",
    "RewardShapedQLearning(mdp, EpsilonGreedy(), potential, qfunction).execute()\n",
    "# RewardShapedQLearning(mdp, EpsilonGreedy(), potential, qfunction).execute(visualize=True)\n",
    "policy = qfunction.extract_policy(mdp)\n",
    "mdp.visualise_q_function(qfunction)\n",
    "mdp.visualise_policy(policy)\n",
    "reward_shaped_rewards = mdp.get_rewards()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we compare this with Q-learning without reward shaping. Notice that many updates to the Q function yield no change from the initial Q function (all zeros). Why is this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mdp = GridWorld(width = 15, height = 12, goals = [((14,11), 1), ((13,11), -1)])\n",
    "mdp = GridWorld(width = 10, height = 7, goals = [((9,6), 1), ((8,6), -1)])\n",
    "qfunction = QTable()\n",
    "QLearning(mdp, EpsilonGreedy(), qfunction).execute()\n",
    "policy = qfunction.extract_policy(mdp)\n",
    "mdp.visualise_q_function(qfunction)\n",
    "mdp.visualise_policy(policy)\n",
    "q_learning_rewards = mdp.get_rewards()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we plot the average episode length during training, we see that reward shaping reduces the length of the early episodes because it has knowledge nudging it towards the goal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Plot:\n",
    "    @staticmethod\n",
    "    def plot_episode_length(labels, data):\n",
    "        # Assuming 'data' is a list of lists with per-step rewards as floats\n",
    "        # And 'labels' is a list of the names of the strategies\n",
    "\n",
    "        # Check if 'labels' and 'data' lengths match\n",
    "        if len(labels) != len(data):\n",
    "            raise ValueError(\"Labels and data must have the same length\")\n",
    "\n",
    "        # Set the background color\n",
    "        plt.figure(facecolor='whitesmoke')\n",
    "        ax = plt.axes()\n",
    "        \n",
    "        # Setting the background color of the\n",
    "        # plot using set_facecolor() method\n",
    "        ax.set_facecolor(\"gainsboro\")\n",
    "\n",
    "        # Plotting each strategy\n",
    "        for i, sublist in enumerate(data):\n",
    "            epsiode_length = [len(subsublist) for subsublist in sublist]\n",
    "            iterations = range(len(sublist))\n",
    "            plt.plot(iterations, epsiode_length, label=labels[i])\n",
    "\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Episode length')\n",
    "        plt.title('Episode Length Over Time')\n",
    "        plt.legend()\n",
    "        \n",
    "        # Add gridlines\n",
    "        plt.grid(True, color=\"white\")\n",
    "        \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tests.plot import Plot\n",
    "\n",
    "Plot.plot_episode_length(\n",
    "    [\"Tabular Q-learning\", \"Reward shaping\"],\n",
    "    [q_learning_rewards, reward_shaped_rewards],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A bad potential function\n",
    "Consider a bad potential function, that nudges the agent in exactly the opposite direction of the 'good' potential function above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return the opposite of the good potential function\n",
    "class BadGridWorldPotentialFunction(GridWorldPotentialFunction):\n",
    "    def get_potential(self, state):\n",
    "        return -super().get_potential(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mdp = GridWorld(width = 10, height = 7, goals = [((9,6), 1), ((8,6), -1)])\n",
    "qfunction = QTable()\n",
    "potential = BadGridWorldPotentialFunction(mdp)\n",
    "RewardShapedQLearning(mdp, EpsilonGreedy(), potential, qfunction).execute()\n",
    "policy = qfunction.extract_policy(mdp)\n",
    "mdp.visualise_q_function(qfunction)\n",
    "mdp.visualise_policy(policy)\n",
    "bad_reward_shaped_rewards = mdp.get_rewards()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examining the episode lengths, we can see that the agent learns the shortest path even more slowly than providing no information to the agent at all!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tests.plot import Plot\n",
    "\n",
    "Plot.plot_episode_length(\n",
    "    [\"Tabular Q-learning\", \"Reward shaping\", \"Bad reward shaping\"],\n",
    "    [q_learning_rewards, reward_shaped_rewards, bad_reward_shaped_rewards],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-potential-based reward shaping (a.k.a., reward tuning)\n",
    "Reward tuning can be a highly effective strategy because it provides the designer with full flexibility to encode expert domain knowledge into the reward function. The following example provides a cautionary tale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_free_reinforcement_learner import ModelFreeReinforcementLearner\n",
    "from qlearning import QLearning\n",
    "\n",
    "\n",
    "class RewardTunedQLearning(QLearning):\n",
    "    def __init__(self, mdp, bandit, tuning, qfunction, alpha=0.1):\n",
    "        super().__init__(mdp, bandit, qfunction=qfunction, alpha=alpha)\n",
    "        self.tuning = tuning\n",
    "\n",
    "    def get_delta(self, reward, q_value, state, next_state, next_action):\n",
    "        next_state_value = self.state_value(next_state, next_action)\n",
    "        tuning = self.tuning.get_tuning(state, next_state, self.mdp.discount_factor)\n",
    "        delta = reward + tuning + self.mdp.discount_factor * next_state_value - q_value\n",
    "        return delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see that reward tuning is strictly more general than reward shaping, we can implement the earlier potential-based reward function as reward tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return the opposite of the good potential function\n",
    "class PotentialFunctionRewardTuning(GridWorldPotentialFunction):\n",
    "    def get_tuning(self, state, next_state, discount_factor):\n",
    "        return discount_factor * super().get_potential(next_state) - super().get_potential(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mdp = GridWorld(width = 10, height = 7, goals = [((9,6), 1), ((8,6), -1)])\n",
    "qfunction = QTable()\n",
    "tuning = PotentialFunctionRewardTuning(mdp)\n",
    "RewardTunedQLearning(mdp, EpsilonGreedy(), tuning, qfunction).execute()\n",
    "policy = qfunction.extract_policy(mdp)\n",
    "reward_shaped_rewards = mdp.get_rewards()\n",
    "\n",
    "mdp = GridWorld(width = 10, height = 7, goals = [((9,6), 1), ((8,6), -1)])\n",
    "qfunction = QTable()\n",
    "QLearning(mdp, EpsilonGreedy(), qfunction).execute()\n",
    "policy = qfunction.extract_policy(mdp)\n",
    "q_learning_rewards = mdp.get_rewards()\n",
    "\n",
    "Plot.plot_episode_length(\n",
    "    [\"Tabular Q-learning\", \"Reward shaping\"],\n",
    "    [q_learning_rewards, reward_shaped_rewards],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we can also implement non-potential-based reward functions. The following (small!) reward tuning introduces a reward loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return the opposite of the good potential function\n",
    "class NotAPotentialFunction(GridWorldPotentialFunction):\n",
    "    def get_tuning(self, state, next_state, discount_factor):\n",
    "        default = discount_factor * super().get_potential(next_state) - super().get_potential(state)\n",
    "        tune_states = [(0,0), (1,0), (2,0), (2,1), (2,2), (1,2), (0,2), (0,1)]\n",
    "        for s,s_next in zip(tune_states, tune_states[1:] + [tune_states[0]]):\n",
    "            if state == s and next_state == s_next:\n",
    "                default += 0.05\n",
    "        return default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mdp = GridWorld(width = 10, height = 7, goals = [((9,6), 1), ((8,6), -1)])\n",
    "qfunction = QTable()\n",
    "tuning = NotAPotentialFunction(mdp)\n",
    "RewardTunedQLearning(mdp, EpsilonGreedy(), tuning, qfunction).execute()\n",
    "policy = qfunction.extract_policy(mdp)\n",
    "mdp.visualise_q_function(qfunction)\n",
    "mdp.visualise_policy(policy)\n",
    "reward_shaped_rewards = mdp.get_rewards()\n",
    "\n",
    "mdp = GridWorld(width = 10, height = 7, goals = [((9,6), 1), ((8,6), -1)])\n",
    "qfunction = QTable()\n",
    "QLearning(mdp, EpsilonGreedy(), qfunction).execute()\n",
    "policy = qfunction.extract_policy(mdp)\n",
    "q_learning_rewards = mdp.get_rewards()\n",
    "\n",
    "Plot.plot_episode_length(\n",
    "    [\"Tabular Q-learning\", \"Reward shaping\"],\n",
    "    [q_learning_rewards, reward_shaped_rewards],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A grid too large?\n",
    "Is there a grid too large for the (unshaped) Q-learning agent to learn in 100 iterations? How does potential-based reward shaping fare?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from qtable import QTable\n",
    "from qlearning import QLearning\n",
    "from reward_shaped_qlearning import RewardShapedQLearning\n",
    "from gridworld_potential_function import GridWorldPotentialFunction\n",
    "from multi_armed_bandit.epsilon_greedy import EpsilonGreedy\n",
    "\n",
    "\n",
    "mdp = GridWorld(width = 50, height = 50, goals = [((49,49), 1), ((48,49), -1)])\n",
    "qfunction = QTable()\n",
    "potential = GridWorldPotentialFunction(mdp)\n",
    "RewardShapedQLearning(mdp, EpsilonGreedy(), potential, qfunction).execute()\n",
    "# RewardShapedQLearning(mdp, EpsilonGreedy(), potential, qfunction).execute(visualize=True)\n",
    "policy = qfunction.extract_policy(mdp)\n",
    "reward_shaped_rewards = mdp.get_rewards()\n",
    "\n",
    "mdp = GridWorld(width = 50, height = 50, goals = [((49,49), 1), ((48,49), -1)])\n",
    "qfunction = QTable()\n",
    "QLearning(mdp, EpsilonGreedy(), qfunction).execute()\n",
    "policy = qfunction.extract_policy(mdp)\n",
    "q_learning_rewards = mdp.get_rewards()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tests.plot import Plot\n",
    "\n",
    "Plot.plot_episode_length(\n",
    "    [\"Tabular Q-learning\", \"Reward shaping\"],\n",
    "    [q_learning_rewards, reward_shaped_rewards],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
